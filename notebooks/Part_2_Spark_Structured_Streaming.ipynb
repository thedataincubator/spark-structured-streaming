{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark Structured Streaming\n",
    "<!-- We give an overview of our deep dive into Spark Structured Streaming. -->\n",
    "\n",
    "Recall that we can think of Spark Structured Streaming as generating an infinite dataset against which we write queries.  We'll explore the API fully.\n",
    "\n",
    "- We'll demonstrate how to pull data from different streaming sources like websockets or files.\n",
    "- We'll delve into parsing and structuring data.\n",
    "- We'll demonstrate how to run queries against this data using the structured streaming API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sparkDummy = spark\n",
    "import sparkDummy.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socket Structured Streaming Example\n",
    "<!-- We demonstrate how to use sockets to listen on a fixed port and how to set up a simple netcat server to broadcast to that port. -->\n",
    "\n",
    "We're going to stream one of Shakespeare's most famous poems on a fixed port and listen for it using Spark.  We set up a streaming server to broadcast the file one line at a time using [Broadcast.scala](/edit/Broadcast.scala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// create a stream and listen on a port\n",
    "\n",
    "def createStream(port: Int, duration: Int) {\n",
    "    val lines = (spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\", \"localhost\")\n",
    "        .option(\"port\", port)\n",
    "        .load())\n",
    "\n",
    "    val words = (lines\n",
    "        .as[String]\n",
    "        .flatMap(_.split(\"\\\\s+\")))\n",
    "\n",
    "    val wordCounts = (words\n",
    "        .groupByKey(_.toLowerCase)\n",
    "        .count()\n",
    "        .orderBy($\"count(1)\" desc))\n",
    "\n",
    "    val query = (wordCounts.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .start\n",
    "        .awaitTermination(duration))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// run `nc -lk 12341` in bash and start typing!\n",
    "\n",
    "createStream(12341, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**Exercise:** type \"foo\", \"foo bar\",  and \"bar\", and watch the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netcat Socket Structured Streaming Example\n",
    "<!-- We will demonstrate how to write a small Scala script to broadcast a file to that port and how to invoke bash commands from Scala repls and notebooks for testing the broadcast server -->\n",
    "\n",
    "We can also broadcast on Unix's `netcat` to broadcast a stream on a fixed port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"more data/summer.txt\" ! // run bash command using bang after a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val port = 12342\n",
    "\n",
    "// Broadcast file on port one line at a time\n",
    "(new Thread {\n",
    "    override def run {\n",
    "        s\"scala Broadcast.scala ${port} data/summer.txt\" !\n",
    "    }\n",
    "}).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "createStream(port, 12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Parsing Data\n",
    "<!-- Most manipulations we do will involve structuring data.  We demonstrate how to use case classes and Scala Reflection to easily structure our data and account for missing or incomplete fields. -->\n",
    "\n",
    "Much as with datasets, we can use a `case class` to represent rows of data.  The case class's attributes correspond to the json field names or (as in this case) the CSV column names.\n",
    "\n",
    "However, unlike with datasets, we cannot ask the reader to infer the schema.  Instead, we will use `ScalaReflection` to generate a schema for our case class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"cat data/people/1.csv\" ! // run bash command using bang after a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "\n",
    "case class Person(\n",
    "    name: String,\n",
    "    city: String,\n",
    "    country: String,\n",
    "    age: Option[Int]\n",
    ")\n",
    "\n",
    "// create schema for parsing data\n",
    "val caseSchema = (ScalaReflection\n",
    "    .schemaFor[Person]\n",
    "    .dataType\n",
    "    .asInstanceOf[StructType])\n",
    "\n",
    "val peopleStream = (spark.readStream\n",
    "    .schema(caseSchema)\n",
    "    .option(\"header\", true)  // Headers are matched to Person properties\n",
    "    .option(\"maxFilesPerTrigger\", 1)  // each file is read in a separate batch\n",
    "    .csv(\"data/people/\")  // load a CSV file\n",
    "    .as[Person])\n",
    "  \n",
    "(peopleStream.writeStream\n",
    "    .outputMode(\"append\")  // write results to screen\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "- What would happen if age were not optional?\n",
    "- What if the age were \"five\" instead of \"5\"?\n",
    "- What if one of the records was missing an \"age\" record?\n",
    "- There's also a `.json` method that we could use in lieu of the `.csv` method. Can you guess the json schema that this code would read?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Columns in Structured Streaming\n",
    "<!-- Structured Streaming makes heavy use of Column objects for manipulating data.  In this section, we explain various ways in which the Column objects can be constructed from columns in our structured stream or by combining other columns. -->\n",
    "\n",
    "Datasets use a dataframe syntax to refer to columns (which are themselves `Column` objects).  There are a number of ways to do this:\n",
    "- `peopleStream(\"country\")`\n",
    "- `peopleStream.col(\"country\")`\n",
    "- `$\"country\"`\n",
    "- `'country`\n",
    "\n",
    "The first two are more explicit as they tell Spark which data stream to use.  This is useful in joins when we want to specify the table more explicitly.  The second two are more implicit as they do not specify the data stream.  These are more useful for single datastream operations.  The symbols need to be imported from `spark.implicits`.\n",
    "\n",
    "There are actually multiple ways to construct columns:\n",
    "- The above allows us to reference `Column`s already in a dataframe.\n",
    "- We can also construct a `Column` from other `Column`s using binary operators like `===` (equality), `>`, `<=`, `.plus`, `-`, `.startsWith`, or `&&`, depending on the underlying value of the column.\n",
    "- Finally, we can rename the columns (keeping the values) with the operator `as`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Filtering Columns Using Structured Streaming\n",
    "<!-- We'll demonstrate how to select and filter columns using Structured Streaming. -->\n",
    "\n",
    "We'll demonstrate these using the `select` method, which takes any non-zero number of `Column` arguments and returns a dataframe with those arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(peopleStream.select(\n",
    "    $\"country\" === \"UK\" as \"in_UK\",\n",
    "    $\"age\" <= 30 as \"under_30\",\n",
    "    'country startsWith \"U\" as \"U_Country\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")  // write results to screen\n",
    "        .format(\"console\")\n",
    "        .start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(peopleStream.filter($\"age\" === 22)\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  // write results to screen\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercises:**\n",
    "\n",
    "1. Select the column \"age + 1\".\n",
    "1. Select a column that returns true if the user is a Londoner who is under 30 as \"Young_Londoner\".\n",
    "1. Filter for when the age is no less than 22.\n",
    "1. Filter for the city being \"London\".\n",
    "1. Filter for Americans under the age of 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregation in Structured Streaming\n",
    "<!-- We'll demonstrate how to perform groupBy and data aggregation in Structured Streaming.  We will also demonstrate how to use groupBy on multiple columns. -->\n",
    "\n",
    "We can use groupBy and aggregation as we would in SQL.\n",
    "\n",
    "- `groupBy` takes one or more `Column`s along which to groupBy.\n",
    "- The resulting object supports various built-in aggregation functions (`avg`, `mean`, `min`, `max`, `sum`) which take one or more string column names along which to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(peopleStream.groupBy('country)\n",
    "    .mean(\"age\")\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex aggregations, we can use `.agg`, which takes columns with aggregations.  Notice that we can reuse the keyword `as`, as well as other binary column operators from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(peopleStream.groupBy('city)\n",
    "    .agg(first(\"country\") as \"country\", count(\"age\"))\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Add the average age of each city to the above query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Structured Stream with Datasets\n",
    "<!-- One of the best features of Structured Stream is the ability to natively join batch data with a Structured Stream. -->\n",
    "\n",
    "We can join datastreams with datasets.  Remember: both of these are distributed datasets and one is being streamed -- that's a lot of semantics for a simple `.join` operator!\n",
    "\n",
    "Below, we take a fixed user table and join it in with a stream of transactions in a fictitious poultry ecommerce website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case class User(id: Int, name: String, email: String, country: String)\n",
    "case class Transaction(userid: Int, product: String, cost: Double)\n",
    "\n",
    "// A user dataset\n",
    "// Notice that we do not have to provide a schema\n",
    "// We can simply infer it\n",
    "val users = (spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", true)\n",
    "    .csv(\"data/users.csv\")\n",
    "    .as[User]\n",
    ")\n",
    "\n",
    "val transactionSchema = (ScalaReflection\n",
    "    .schemaFor[Transaction]\n",
    "    .dataType\n",
    "    .asInstanceOf[StructType]\n",
    ")\n",
    "  \n",
    "// A stream of transactions\n",
    "val transactionStream = (spark.readStream\n",
    "    .schema(transactionSchema)\n",
    "    .option(\"header\", true)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"data/transactions/*.csv\")\n",
    "    .as[Transaction]\n",
    ")\n",
    "\n",
    "// Join transaction stream with user dataset\n",
    "val spendingByCountry = (transactionStream\n",
    "    .join(users, users(\"id\") === transactionStream(\"userid\"))\n",
    "    .groupBy($\"country\")\n",
    "    .agg(sum($\"cost\")) as \"spending\")\n",
    "    \n",
    "// Print result\n",
    "(spendingByCountry.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "- Show sales by product rather than country.\n",
    "- Show sales by both product and country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries in Spark Structured Streaming\n",
    "<!-- Spark also has an escape hatch into SQL queries that allows users to write familiar SQL queries against Structured Streams. -->\n",
    "\n",
    "Finally we can use the method `createOrReplaceTempView` to publish streams (and static datasets) as SQL tables.  We can then query the resulting table using SQL and stream the output as we would with any other datastream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Publish SQL table\n",
    "peopleStream.createOrReplaceTempView(\"peopleTable\")\n",
    "\n",
    "// SQL query\n",
    "val query = spark.sql(\"SELECT country, avg(age) FROM peopleTable GROUP BY country\")\n",
    "\n",
    "// Output\n",
    "(query.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "- Use the SQL syntax to filter for Londoners under 40 years.\n",
    "- Use the SQL syntax to join the user table and transaction stream to get transactions by country and product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo-text.jpg\" width=\"20%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
